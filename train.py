# -*- coding: utf-8 -*-
"""SurfaceReconstruction_reg_multiC.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18T5Kl6W0RsyncM0Uq-Le0Jd7FcvzrvT5
"""

# Commented out IPython magic to ensure Python compatibility.
# show the GPU
# %ls

import os
import torch
import torch.nn.functional as F
import yaml
from tqdm import tqdm
import numpy as np
import datasets
from network import Network
import utils.argparseFromFile as argparse
from utils.utils import wblue, wgreen
from utils.loss import UShareLoss
import utils.metrics as metrics
from sklearn.metrics import confusion_matrix

from torch.utils.tensorboard import SummaryWriter

import logging

print_no_end= lambda s: print(s, end="", flush=True)

def save_config_file(config, filename):
    with open(filename, 'w') as outfile:
        yaml.dump(config, outfile, default_flow_style=False)


def main(config):

    config = eval(str(config))    
    disable_log = (config["log_mode"] != "interactive")
    device = torch.device(config['device'])
    n_points = config["manifold_points"]


    # create the network
    logging.info("Creating the network")
    net = Network(latent_size=config["latent_size"], VAE=config["vae"])

    # create the save directory (if needed)
    logging.info("Saving the configuration file")
    config["save_dir"] = os.path.join(config["save_dir"], 
        f"{config['dataset_name']}_{config['experiment_name']}_{config['manifold_points']}_{config['non_manifold_points']}_filter{config['filter_name']}")
    os.makedirs(config["save_dir"], exist_ok=True)
    save_config_file(config, os.path.join(config['save_dir'], "config.yaml"))

    # create the summary writer
    logging.info("Creating tensorboard summary writer")
    writer = SummaryWriter(log_dir=os.path.join(config["save_dir"], "logs_tb"))

    # resume or not
    
    epoch_start = 0
    resume=False
    if config["init_with"] is not None:
        logging.info("Init with weights")
        ckpt = torch.load(config["init_with"])
        net.load_state_dict(ckpt["network"])
    elif config["resume"]:
        logging.info("Resume with weights")
        ckpt = torch.load(os.path.join(config["save_dir"], "checkpoint.pth"))
        net.load_state_dict(ckpt["network"])
        epoch_start = ckpt["iter"]
        resume=True
    else:
        ckpt = None
        try:
            os.remove(os.path.join(config["save_dir"], "logs.txt"))
        except:
            logging.info("Cleaning previous files -- no log file to delete")


    # set net to device
    net.to(device)
    
    # create the dataset and dataloader
    logging.info("Dataset -- creating the dataset")
    dataset = getattr(datasets, config["dataset_name"])(config)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=config["batch_size"], shuffle=True, num_workers=config["threads"])

    # create the optimizer
    logging.info("Optimizer -- Creating the optimizer")
    optimizer_all = torch.optim.Adam(net.parameters(), lr=config["lr_start"])
    if resume:
        print("Optimize -- loading weights")
        optimizer_all.load_state_dict(ckpt["optimizer"])

    logging.info("Loss -- creating the loss")
    ushape_loss = UShareLoss(config["alpha_diff"], config["alpha_same"],
                        config["alpha_box"], config["alpha_code_regul"])


    has_non_manifold_gt = False

    for epoch in range(epoch_start, config["num_epochs"]):

        description_str=""
        total_loss = 0
        total_loss_diff = 0
        total_loss_same = 0
        total_iou = 0 # used only if IoU is computable --> e.g. ShapeNet
        batch_count = 0
        t = tqdm(data_loader, ncols=130, disable=disable_log)
        for data in t:
            
            # prepare the datas
            if "input_voxels" in config and config["input_voxels"]:
                manifold_points = data['voxels'].to(device)
            else:
                manifold_points = data['manifold_points'].to(device)

            non_manifold_points = data["non_manifold_points"].to(device)
            ids_pts2pts = data["ids_pts2pts"].to(device)


            # et optimizer to zero
            optimizer_all.zero_grad()

            outputs_space, outputs_space_latent_reg = net(non_manifold_points,manifold_points, return_latent_reg=True)

            loss, loss_values = ushape_loss(outputs_space, n_points, ids_pts2pts, return_all_losses_values=True)
                
            if outputs_space_latent_reg is not None:
                loss = loss + outputs_space_latent_reg.mean()
            
            loss.backward()
            optimizer_all.step()

            # loss values is disctionnary with the losses as float
            total_loss_same += loss_values["loss_same"]
            total_loss_diff += loss_values["loss_diff"]
            total_loss += loss_values["loss"]

            batch_count += 1

            description_str = f"{epoch} | L {total_loss/batch_count:.2e}  | Ldiff {total_loss_diff/batch_count:.2e} | Lsame {total_loss_same/batch_count:.2e}"

            if "non_manifold_gt" in data:
                has_non_manifold_gt = True
                occupancies = data["non_manifold_gt"].cpu().numpy()
                outputs_non_manifold = outputs_space[:,-(8+occupancies.shape[1]):-8]
                outputs_non_manifold = torch.sigmoid(outputs_non_manifold).detach().cpu().numpy()
                cm1 = confusion_matrix(
                    occupancies.ravel(), 
                    (outputs_non_manifold.ravel()>0.5).astype(np.int64),
                    labels=list(range(2))
                    )
                iou1 = metrics.stats_iou_per_class(cm1)[0]
                cm2 = confusion_matrix(
                    occupancies.ravel(), 
                    (outputs_non_manifold.ravel()<0.5).astype(np.int64),
                    labels=list(range(2))
                    )
                iou2 = metrics.stats_iou_per_class(cm2)[0]

                total_iou += max(iou1, iou2)

                description_str += f" | IoU {total_iou/batch_count:.2e}"

            t.set_description_str(wblue(description_str))

        total_loss /= batch_count
        total_loss_diff /= batch_count
        total_loss_same /= batch_count
        total_iou /= batch_count

        # printing the logs if needed
        if disable_log:
            print(description_str)

        # saving the network
        save_dict = { 
            "optimizer":optimizer_all.state_dict(),
            "network": net.state_dict(),
            "iter": epoch,
            "loss": total_loss,
            }
        torch.save(save_dict, os.path.join(config["save_dir"], "checkpoint.pth"))
        
        if (config["save_every"] is not None) and (epoch > 0) and (epoch % config["save_every"]==0):
            torch.save(save_dict, os.path.join(config["save_dir"], f"checkpoint_{epoch}.pth"))

        log_str = f"{epoch} {total_loss:.4e} {total_loss_diff:.4e} {total_loss_same:.4e}"
        if has_non_manifold_gt:
            log_str += f" {total_iou:.4e}"
        log_str += "\n"

        # save the logs
        with open(os.path.join(config['save_dir'], "logs.txt"), "a") as myfile:
            myfile.write(log_str)

        # tensorboard logging
        writer.add_scalar('Loss/loss', total_loss, epoch)
        writer.add_scalar('Loss/different_sides', total_loss_diff, epoch)
        writer.add_scalar('Loss/same_side', total_loss_same, epoch)

        # tensorboard metrics
        if has_non_manifold_gt:
            writer.add_scalar('Metrics/IoU', total_iou, epoch)

        # saving the points
        outputs_space = torch.sigmoid(outputs_space) * 2 -1
        outputs_space = outputs_space.detach().cpu().numpy()
        pts = non_manifold_points.detach().cpu().numpy()
        pts = np.concatenate([pts[0], np.expand_dims(outputs_space[0], axis=1)], axis=1)
        np.savetxt(os.path.join(config["save_dir"], f"points.txt"), pts)


if __name__ == "__main__":


    parser = argparse.ArgumentParserFromFile(description='Process some integers.')
    parser.add_argument('--config_default', type=str, default="configs/config_default.yaml")
    parser.add_argument('--config', '-c', type=str, default=None)
    parser.update_file_arg_names(["config_default", "config"])


    config = parser.parse(use_unknown=True)

    logging.getLogger().setLevel(config["logging"])
    if config["logging"] == "DEBUG":
        config["threads"] = 0
    
    main(config)